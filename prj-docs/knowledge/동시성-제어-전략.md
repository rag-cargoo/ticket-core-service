# 동시성 제어 전략 (Concurrency Control Strategy)

<!-- DOC_META_START -->
> [!NOTE]
> - **Created At**: `2026-02-08 23:07:57`
> - **Updated At**: `2026-02-11 09:34:00`
<!-- DOC_META_END -->

<!-- DOC_TOC_START -->
## 단계 목차 (Step Index)
---
> [!TIP]
> - Step 0: 문제 상황 (락 미적용)
> - Step 1: 낙관적 락 (Optimistic Lock)
> - Step 2: 비관적 락 (Pessimistic Lock)
> - Step 3: Redis 분산 락 (Distributed Lock)
> - Step 3-1: 첫 번째 시도 (실패 사례 - 30/30 성공)
> - Step 3-2: 두 번째 시도 (성공 사례 - Facade 패턴 도입)
> - Step 4: Kafka 기반 비동기 예약 시스템 도입 (Next Phase)
> - Step 5: Redis Sorted Set 기반 실시간 대기열
> - Step 6: 유입량 제어 전략 (Throttling)
> - Step 7: SSE 기반 실시간 순번 자동 푸시
> - Step 8: k6 성능 기준선 확정 및 병목 제거
<!-- DOC_TOC_END -->

**Purpose**: 선착순 좌석 예약 시스템에서 발생하는 데이터 정합성 문제 해결 및 성능 최적화 전략 기록
**Date**: 2026-02-05

## 0. 이론 배경: 낙관적 락 vs 비관적 락 (Theory)
---

> [!NOTE]
> **Comparison Summary**
>
> | 특징 | 낙관적 락 (Optimistic Lock) | 비관적 락 (Pessimistic Lock) |
> | :--- | :--- | :--- |
> | **기본 사상** | "충돌은 드물 거야. 충돌 나면 그때 해결하자." | "충돌은 빈번할 거야. 아예 못 건드리게 막자." |
> | **구현 방식** | **애플리케이션 로직**: 버전 컬럼(`@Version`) 비교. | **DB 쿼리**: `SELECT ... FOR UPDATE`로 물리적 락. |
> | **성능 (DB)** | **높음** (락 대기 없음) | **낮음** (대기 발생, 데드락 위험) |
> | **충돌 처리** | 예외 발생 (`Exception`) -> 롤백 -> 재시도 필요 | 대기 (`Wait`) -> 순차 처리 -> 성공 |
> | **대표 예시** | 게시판 수정, 프로필 업데이트 | 선착순 예매, 재고 차감, 계좌 이체 |

> ### 실험 기록 순서 (Progress)
> 1. [Step 0: 문제 상황 (락 미적용)](#step-0-문제-상황-락-미적용)
> 2. [Step 1: 낙관적 락 (Optimistic Lock)](#step-1-낙관적-락-optimistic-lock)
> 3. [Step 2: 비관적 락 (Pessimistic Lock)](#step-2-비관적-락-pessimistic-lock)
> 4. [Step 3: Redis 분산 락 (Distributed Lock)](#step-3-redis-분산-락-distributed-lock)
> 5. [Step 4: Kafka 기반 비동기 대기열 (Async Queue)](#step-4-kafka-기반-비동기-대기열-async-queue)
> 6. [Step 5: Redis Sorted Set 기반 실시간 대기열](#step-5-redis-sorted-set-기반-실시간-대기열)
> 7. [Step 6: 유입량 제어 전략 (Throttling)](#step-6-유입량-제어-전략-throttling)
> 8. [Step 7: SSE 기반 실시간 순번 자동 푸시](#step-7-sse-기반-실시간-순번-자동-푸시)
> 9. [Step 8: k6 성능 기준선 확정 및 병목 제거](#step-8-k6-성능-기준선-확정-및-병목-제거)

<br><br>

## Step 0: 문제 상황 (락 미적용)
---

> [!WARNING]
> **데이터 소실 위험**: 아무런 보호 장치가 없을 경우, 마지막에 들어온 요청이 이전의 모든 데이터를 덮어쓰는 Race Condition이 발생합니다.
>
> ### 0.1. 실험 시나리오 (Experimental Scenario)
> - **대상**: 1개 좌석 (Seat ID: 1)
> - **부하**: 30명의 스레드가 동시에 예약 API 호출
>
> ### 0.2. 잘못된 구현 방식 (The Wrong Way)
> 아무런 보호 장치 없이 일반적인 조회와 수정을 반복하는 방식입니다.
>
> ```java
> // [Service]
> @Transactional
> public void reserve(Long seatId) {
>   Seat seat = seatRepository.findById(seatId).orElseThrow(); // 누구나 동시에 조회 가능
>   seat.reserve(); // 각자 메모리 상에서 상태 변경
>   // 커밋 시점에 마지막에 들어온 요청이 덮어쓰거나 중복 생성됨 (Race Condition)
> }
> ```
>
> ### 0.3. 실험 결과 (실패)
> 데이터 정합성이 완전히 깨지는 현상이 발생함. (중복 예약 발생)
>
> #### [수치 결과]
>
> | 항목 | 결과값 | 비고 |
> | :--- | :--- | :--- |
> | **성공 횟수** | 10건 | **치명적 오류**: 1건이어야 함 |
> | **최종 예약 건수** | **10건** | **중복 예약 발생 (Race Condition)** |

<br><br>

## Step 1: 낙관적 락 (Optimistic Lock)
---

> [!NOTE]
> **애플리케이션 레벨 제어**: 엔티티의 버전을 체크하여 충돌을 감지합니다. DB 부하가 적지만 충돌 시 재시도 로직이 필요합니다.
>
> ### 1.1. 실험 시나리오 (Experimental Scenario)
> - **테스트 코드**: `동시성_테스트_1_낙관적_락.java`
> - **대상**: 1개 좌석 (Seat ID: 1)
> - **부하**: 30명의 스레드가 동시에 예약 API 호출
>
> ### 1.2. 구현 방법 (How to Apply)
> 엔티티에 `@Version` 컬럼을 추가하는 것만으로 애플리케이션 레벨의 체크 로직이 활성화됩니다.
>
> ```java
> // [Entity]
> @Version // JPA가 제공하는 낙관적 락 메커니즘
> private Long version;
> ```
>
> ### 1.3. 동작 원리 및 SQL
> 수정 쿼리 시점에 자동으로 버전 체크 조건이 붙습니다.
>
> ```sql
> update seats
> set status='RESERVED', version=1
> where id=1 and version=0; -- 처음 읽은 버전이 0일 때만 성공
> ```
>
> ### 1.4. 실험 결과 및 분석
> - **성공 횟수**: 1건
> - **실패 횟수**: 29건 (ObjectOptimisticLockingFailureException 발생)
> - **결론**: 데이터 정합성은 보장하나, 충돌 시 사용자에게 에러를 반환하므로 선착순 예매에는 한계가 있음.

<br><br>

## Step 2: 비관적 락 (Pessimistic Lock)
---

> [!NOTE]
> **DB 엔진 레벨 제어**: `SELECT ... FOR UPDATE`를 통해 데이터를 즉시 잠급니다. 강력한 정합성을 보장합니다.
>
> ### 2.1. 실험 시나리오 (Experimental Scenario)
> - **테스트 코드**: `동시성_테스트_2_비관적_락.java`
> - **대상**: 1개 좌석 (Seat ID: 1)
> - **부하**: 30명의 스레드가 동시에 예약 API 호출
>
> ### 2.2. 구현 방법 (How to Apply)
> Spring Data JPA에서 비관적 락을 거는 방법은 크게 두 가지가 있습니다.
>
> #### 방법 A: @Lock 어노테이션 사용 (권장)
> JPA가 제공하는 `@Lock` 어노테이션을 사용하여 쿼리 생성 시점에 락 모드를 지정합니다. 이 방식을 쓰면 JPA가 SQL 끝에 자동으로 `FOR UPDATE`를 추가해 줍니다.
>
> ```java
> // [Repository]
> public interface SeatRepository extends JpaRepository<Seat, Long> {
>
>   @Lock(LockModeType.PESSIMISTIC_WRITE) // 핵심: 비관적 락 모드 지정
>   @Query("SELECT s FROM Seat s WHERE s.id = :id")
>   Optional<Seat> findByIdWithPessimisticLock(@Param("id") Long id);
> }
> ```
>
> #### 방법 B: 쿼리에 직접 명시
> 어노테이션을 사용하지 않고, SQL 또는 JPQL 끝에 직접 락 키워드를 작성합니다. 하지만 JPA 환경에서는 일관성을 위해 `@Lock` 사용을 권장합니다.
>
> ### 2.3. 동작 원리 및 SQL
> 데이터를 읽는 시점부터 DB가 다른 스레드의 접근을 차단합니다.
>
> ```sql
> /* 방법 A 적용 시 실행되는 실제 SQL */
> select s1_0.id, ...
> from seats s1_0
> where s1_0.id=?
> for update; -- DB 엔진이 해당 행(Row)을 꽉 잡음 (자물쇠)
> ```
>
> ### 2.4. 결론 (Conclusion)
> - **비관적 락은 쿼리로 해결한다**: 코드 로직이 아니라 DB 엔진의 힘을 빌려 **줄을 세우는 방식**이다.
> - **장점**: 낙관적 락보다 정합성이 강력하고, 불필요한 예외 재시도 로직이 필요 없다.
> - **단점**: 대기 시간이 길어지면 DB 전체 성능에 영향을 준다.

<br><br>

## 락(Lock)만으로는 해결할 수 없는 현실적인 문제
---

> [!WARNING]
> 낙관적 락과 비관적 락을 통해 데이터 정합성은 확보했으나, 대규모 트래픽(예: 10,000명 동시 접속) 상황에서는 다음과 같은 **치명적인 한계**가 발생한다.
>
> ### 1. 사용자 경험(UX)의 파괴: "만 명 중 한 명만 성공"
> - 현재 방식은 1명이 락을 잡고 있는 동안 나머지는 대기하거나 에러를 받는다.
> - 만 명의 사용자 중 9,999명은 "이미 예약된 좌석입니다"라는 무뚝뚝한 실패 메시지를 받고 다시 광클을 시도해야 한다. (광클 전쟁 유발)
>
> ### 2. 서버 자원 고갈: "DB 커넥션 풀 마비"
> - 비관적 락(`FOR UPDATE`)은 DB 커넥션을 점유한 채로 대기한다.
> - 동시 요청이 많아지면 모든 커넥션이 락 대기에 빠지게 되어, 예약과 상관없는 다른 API(공연 조회 등)까지 모두 먹통이 되는 **'서버 마비'** 상태에 이른다.
>
> ### 해결의 방향: Redis 분산 락과 대기열
> - **체력 강화 (Step 3)**: DB보다 훨씬 가볍고 빠른 **Redis**에서 락을 처리하여 DB 부하를 원천 차단한다.
> - **질서 확립 (Phase 3)**: 단순히 실패시키는 것이 아니라, **대기열(Queue)**을 도입하여 사용자에게 대기 순번을 부여하고 순차적으로 처리하는 UX를 제공한다.

<br><br>

## Step 3: Redis 분산 락 (Distributed Lock)
---

> [!TIP]
> **체력 강화**: DB 대신 초고속 Redis에서 락을 처리하여 DB 부하를 원천 차단하고 분산 환경에서의 정합성을 보장합니다.
>
> ### Step 3-1: 첫 번째 시도 (실패 사례 - 30/30 성공)
>
> #### 3.1.1. 실험 시나리오
> - **테스트 코드**: `동시성_테스트_3_분산_락.java`
> - **부하**: 30명의 스레드가 동시에 예약 API 호출
> - **결과**: **30명 전원 예약 성공 (중복 예약 발생)**
>
> #### 3.1.2. 원인 분석
> Redis 락을 획득했음에도 불구하고 왜 정합성이 깨졌을까? 여기에는 두 가지 치명적인 함정이 있었다.
>
> **함정 1: 락 해제 시점 vs 트랜잭션 커밋 시점**
> - **문제**: Redis 락은 로직이 끝나자마자 `finally` 블록에서 해제된다. 하지만 `@Transactional`에 의한 DB 커밋은 그 이후에 발생한다.
> - **현상**: Thread A가 락을 해제하는 순간, 아직 DB에는 데이터가 반영(Commit)되지 않았다. 이때 대기하던 Thread B가 즉시 락을 잡고 진입하면, B는 여전히 `AVAILABLE` 상태의 데이터를 읽게 된다.
>
> **함정 2: 스프링 프록시의 한계 (Internal Call)**
> - **문제**: `createReservationWithDistributedLock` 내부에서 `this.createReservation()`을 호출하면, 스프링의 AOP 프록시가 작동하지 않아 `@Transactional`이 무시된다.
> - **현상**: 각 DB 작업이 개별 트랜잭션으로 돌거나 커밋 시점이 모호해져서 락의 보호를 전혀 받지 못하게 된다.
>
> #### 3.1.3. 교훈 (Lesson Learned)
> **"분산 락의 해제 시점은 반드시 트랜잭션의 커밋 시점보다 늦어야 한다."**
> 또한, 트랜잭션 전파를 위해 별도의 Facade 클래스를 두거나 스프링 빈의 외부 호출을 이용해야 한다.
>
> ### Step 3-2: 두 번째 시도 (성공 사례 - Facade 패턴 도입)
>
> #### 3.2.1. Facade(파사드) 패턴이란?
> '건물의 정면'이라는 뜻으로, 내부의 복잡한 로직들을 하나의 겉껍데기 클래스로 감싸서 밖에서는 단순하게 보이게 만드는 디자인 패턴이다.
>
> #### 3.2.2. 왜 Facade가 필요한가? (트랜잭션의 함정)
> 스프링의 `@Transactional`은 메서드가 완전히 끝날 때 DB에 내용을 저장(Commit)한다.
> - **Service 내부에서 락을 걸면**: 메서드 끝에서 락을 먼저 풀고, 그 **다음에** DB 저장이 일어난다. (그 사이 찰나에 다른 놈이 들어와서 사고 발생!)
> - **Facade를 사용하면**:
>   1. Facade에서 락을 잡는다.
>   2. Service의 트랜잭션 메서드를 부른다. (DB 저장까지 완벽히 끝날 때까지 기다림)
>   3. **DB 저장이 확실히 끝난 뒤에** Facade에서 락을 푼다.
>
> #### 3.2.3. 해결 전략: RedissonLockFacade 전체 코드
> 생략 없이 실제 구현된 코드를 통해 락과 트랜잭션의 조율 과정을 확인한다.
>
> ```java
> @Component
> @RequiredArgsConstructor
> public class RedissonLockFacade {
>
>   private final RedissonClient redissonClient;
>   private final ReservationService reservationService;
>
>   public ReservationResponse createReservation(ReservationRequest request) {
>     String lockKey = "lock:seat:" + request.seatId();
>     RLock lock = redissonClient.getLock(lockKey);
>
>     try {
>       // [중요 설계 변경] 짐작에 의한 숫자(Magic Number) 배제
>       // 1. Wait Time: 비즈니스 타임아웃 정책에 따라 설정 (ex: 10초)
>       // 2. Lease Time: -1로 설정하여 Redisson Watchdog(감시견)에게 관리를 위임
>       boolean available = lock.tryLock(10, -1, TimeUnit.SECONDS);
>
>       if (!available) {
>         throw new RuntimeException("락 획득 실패: 서버가 바쁩니다. 잠시 후 다시 시도해주세요.");
>       }
>
>       return reservationService.createReservation(request);
>
>     } catch (InterruptedException e) {
>       Thread.currentThread().interrupt();
>       throw new RuntimeException(e);
>     } finally {
>       if (lock.isHeldByCurrentThread()) {
>         lock.unlock();
>       }
>     }
>   }
> }
> ```
>
> <br>
>
> ### [Special] Magic Number를 배제한 자율적 시스템 설계 (중요)
>
> 이 섹션은 단순히 코드를 짜는 것을 넘어, **"진정한 프로그램이란 무엇인가"**에 대한 엔지니어링 철학을 다룬다.
>
> #### 1. 짐작하는 숫자의 위험성 (Anti-Programming)
> 초기에 작성했던 `lock.tryLock(10, 2, TimeUnit.SECONDS)` 코드에서 `2초`라는 숫자는 개발자의 **'짐작'**에 불과하다.
>
> ##### 나쁜 예시 (Bad Practice: Guessing Numbers)
> ```java
> // 로직이 2초 안에 끝날 것이라고 '짐작'하여 하드코딩
> // 만약 DB가 느려져 2.1초가 걸리면? -> 락이 풀리고 중복 데이터 발생! 정합성 파괴!
> boolean available = lock.tryLock(10, 2, TimeUnit.SECONDS);
> ```
>
> ##### 좋은 예시 (Best Practice: Autonomous Management)
> ```java
> // 1. 대기 시간은 정책(Policy)으로 관리
> private static final long MAX_WAIT_TIME = 10L;
>
> // 2. 점유 시간은 -1로 설정하여 시스템(Watchdog)이 스스로 판단하게 함
> // 로직이 얼마나 걸리든 안전하게 정합성 유지
> boolean available = lock.tryLock(MAX_WAIT_TIME, -1, TimeUnit.SECONDS);
> ```
>
> #### 2. 설정 파일(`application.yml`)조차 하드코딩이다
> 값을 설정 파일로 옮기는 행위는 관리의 편의성을 높일 뿐, 결국 사람이 정한 **'정적 수치'**라는 본질은 변하지 않는다. 진정한 해결책은 시스템이 상황에 맞춰 **'스스로 판단'**하게 만드는 것이다.
>
> #### 3. 자율적 해결책: Redisson Watchdog (감시견)
> 우리는 `leaseTime`을 `-1`로 설정함으로써 이 문제를 우아하게 해결했다.
> - **작동 원리**: 로직이 실행 중인 동안 Redisson이 주기적으로 락의 유효시간을 연장한다.
> - **장점**: 로직이 0.1초 만에 끝나든, 10초가 걸리든 **상황에 맞춰 락 유지 시간을 스스로 조절**한다.
> - **안전성**: 만약 서버가 갑자기 죽어 연장 신호를 못 보내면, 그때서야 락을 해제하여 데드락(Deadlock)을 방지한다.
>
> #### 핵심 교훈: "상황을 단정 짓지 말고, 시스템이 흐르게 하라"
> **"설정값은 최소화하고, 자율적인 메커니즘(Watchdog 등)을 우선시하는 설계가 대규모 분산 시스템에서 살아남는 프로그래밍 방식이다."**
>
> #### 3.2.4. 실험 결과 (성공)
> - **성공 횟수**: 1건
> - **실패 횟수**: 29건
> - **최종 예약 건수**: 1건 (정합성 완벽 유지)
>
> #### 3.2.5. 핵심 동작 원리
> 1. **락 획득**: Thread A가 Redis 락 점유.
> 2. **트랜잭션 실행**: 서비스 계층의 트랜잭션 메서드 호출 및 **DB 커밋 완료**.
> 3. **락 해제**: 트랜잭션이 완전히 종료된 후 Facade에서 락 해제.
> 4. **후속 처리**: Thread B가 진입 시, 이미 커밋된 `RESERVED` 상태를 확인하여 중복 예약 방지.
>
> ### 3.3. 최종 결론 (Conclusion)
> - **비관적 락**은 DB 자원을 많이 소모하지만 설정이 간편하다.
> - **Redis 분산 락**은 설정이 복잡(Facade 필요)하지만, DB 부하를 획득 시점 이전에 차단할 수 있어 **대규모 트래픽에 훨씬 유리**하다.

<br><br>

## 대장정의 다음 단계: 왜 대기열(Kafka)인가?
---

> [!NOTE]
> 지금까지 우리는 **'데이터 정합성'**을 지키는 법을 배웠다. 하지만 여전히 해결되지 않은 숙제가 남아있다.
>
> ### 질문: "만 명 중 한 명만 성공한다면, 나머지 9,999명은 어떻게 되는가?"
> 분산 락은 정합성을 지켜주지만, 실패한 9,999명에게는 "실패"라는 응답만 돌려준다. 사용자들은 성공할 때까지 계속해서 버튼을 누를 것이고(재시도), 이는 서버에 **'무한 재시도 폭격'**으로 이어진다.
>
> ### 해결책: '락(Lock)'에서 '대기열(Queue)'로의 패러다임 전환
> - **락(Lock)**: "한 놈만 들어와! 나머지는 다 꺼져!" (배제와 경쟁)
> - **대기열(Queue)**: "일단 모두 줄 서세요. 번호표 순서대로 처리해 드릴게요." (포용과 질서)
>
> ### Step 4: Kafka 기반 비동기 예약 시스템 도입 (Next Phase)
> 이제 우리는 요청을 즉시 처리하지 않고 **Kafka**라는 완충 지대에 담을 것이다.
> 1. **요청 수집**: 만 명의 요청을 0.1초 만에 Kafka에 쌓는다.
> 2. **번호표 부여**: 사용자에게 실시간으로 대기 순번을 제공한다.
> 3. **비동기 처리**: 서버가 감당 가능한 속도로 메시지를 꺼내어 예약 로직을 수행한다.
>
> **결과적으로, 서버는 죽지 않고 사용자는 화내지 않는 '진정한 고성능 시스템'으로 진화한다.**

<br><br>

## Step 4: Kafka 기반 비동기 대기열 (Async Queue)
---

> [!NOTE]
> **패러다임 전환**: 경쟁 기반의 '락'에서 질서 기반의 '대기열'로 전환하여 서버가 감당 가능한 속도로 부하를 분산 처리합니다.
>
> ### 4.1. 왜 '대기열' 방식인가? (The Paradigm Shift)
> 지금까지의 **락(Lock)** 방식은 "한 놈만 들어와, 나머지는 다 실패야"라는 배제적 방식입니다. 이는 만 명의 사용자가 동시에 접속했을 때 9,999명에게 불쾌한 경험을 주며 서버 자원을 고갈시킵니다.
> - **Before (Blocking)**: 클라이언트가 요청을 보내면 DB 예약이 끝날 때까지 스레드가 대기(Block)함.
> - **After (Event-Driven)**: 클라이언트 요청을 즉시 Kafka에 담고 응답(Accepted). 실제 처리는 서버가 감당 가능한 속도로 나중에 수행.
>
> ### 4.2. 구현 방법 (How to Apply)
>
> #### 4.2.1. 요청 수집 (Producer)
> 사용자의 요청을 이벤트 객체로 감싸 Kafka로 쏘아 올립니다. 이때 **`seatId`를 메시지 키(Key)**로 사용하는 것이 핵심입니다.
>
> ```java
> // [KafkaReservationProducer.java]
> public void send(ReservationEvent event) {
>   // seatId를 키로 설정하여 동일 좌석 요청은 무조건 같은 파티션(순서 보장)으로 전송
>   kafkaTemplate.send(topic, String.valueOf(event.getSeatId()), event);
> }
> ```
>
> #### 4.2.2. 비동기 처리 (Consumer)
> Kafka에서 메시지를 하나씩 꺼내어 실제 예약 로직을 수행합니다. 지정된 `lockType`에 따라 앞선 Step 1~2의 로직을 재사용합니다.
>
> ```java
> // [KafkaReservationConsumer.java]
> @KafkaListener(topics = "ticket-reservation-events", groupId = "ticket-group")
> public void consume(ReservationEvent event) {
>   // 1. 상태를 PROCESSING으로 변경
>   queueService.setStatus(userId, seatId, "PROCESSING");
>
>   // 2. 실제 예약 서비스 호출 (Pessimistic or Optimistic)
>   reservationService.createReservationWithPessimisticLock(request);
>
>   // 3. 성공 시 실시간 알림 전송
>   sseManager.send(userId, seatId, "SUCCESS");
> }
> ```
>
> #### 4.2.3. 실시간 결과 통보 (SSE)
> 사용자가 결과를 기다리지 않도록 서버에서 클라이언트로 데이터를 밀어주는 SSE를 적용했습니다.
>
> ```java
> // [SseEmitterManager.java]
> public void send(Long userId, Long seatId, String status) {
>   String key = userId + ":" + seatId;
>   SseEmitter emitter = emitters.get(key);
>   if (emitter != null) {
>     emitter.send(SseEmitter.event().name("RESERVATION_STATUS").data(status));
>     emitter.complete(); // 임무 완수 후 연결 종료
>   }
> }
> ```
>
> ### 4.3. 코드 변환 대조 (Sync to Async Migration)
> 개발자가 기존 동기 API를 비동기 대기열 방식으로 전환할 때 참고할 수 있도록 코드 변화를 대조합니다.
>
> #### Before: 동기식 요청 처리 (Blocking)
> 클라이언트가 요청을 보내면, DB 작업이 끝날 때까지 서버 스레드가 붙잡혀 있습니다.
>
> ```java
> // [Controller]
> @PostMapping("/v1/optimistic")
> public ResponseEntity<ReservationResponse> createReservation(@RequestBody ReservationRequest request) {
>   // 즉시 서비스 호출 -> DB 작업 완료까지 대기(Block)
>   return ResponseEntity.ok(reservationService.createReservation(request));
> }
> ```
>
> #### After: 비동기 이벤트 기반 처리 (Non-blocking)
> 요청을 받자마자 Kafka에 적재하고 즉시 응답합니다. 실제 처리는 별도 컨슈머 스레드에서 수행됩니다.
>
> ```java
> // [Controller]
> @PostMapping("/v4/queue")
> public ResponseEntity<String> createAsyncReservation(@RequestBody ReservationRequest request) {
>   // 1. Redis에 초기 상태 저장
>   queueService.setStatus(request.userId(), request.seatId(), "PENDING");
>
>   // 2. Kafka로 이벤트 발행 (즉시 리턴)
>   kafkaProducer.send(ReservationEvent.of(request.userId(), request.seatId(), OPTIMISTIC));
>
>   return ResponseEntity.accepted().body("Request Enqueued");
> }
> ```
>
> ### 4.4. 적용 방법 (Step-by-Step Implementation)
> 비동기 전환을 위한 3단계 프로세스를 정의합니다.
> 1. **파라미터의 이벤트화**: 컨트롤러가 받던 DTO 데이터(`userId`, `seatId`)를 Kafka 전송용 객체인 `ReservationEvent`로 캡슐화합니다.
> 2. **프로듀서(Producer) 연동**: 컨트롤러에서 `ReservationService` 의존성을 제거하고, 대신 `KafkaReservationProducer`를 주입받아 이벤트를 발행하도록 수정합니다.
> 3. **비즈니스 로직 이관 (Consumer)**: 기존 컨트롤러가 수행하던 `reservationService.create...()` 호출 코드를 `KafkaReservationConsumer`의 `@KafkaListener` 메서드 내부로 옮깁니다. 이때 처리 결과를 **SSE(SseEmitterManager)**를 통해 클라이언트에 통보하도록 연결합니다.
>
> ### 4.5. 핵심 설계 결정 및 정합성 보장 (ADR)
>
> | 결정 사항 | 설계 내용 | 공학적 이유 |
> | :--- | :--- | :--- |
> | **Partition Key** | `seatId` | 동일 좌석에 대한 경합을 단일 컨슈머 스레드에서 순차적으로 처리하기 위함 (물리적 순서 보장) |
> | **Status Store** | Redis | 비동기 처리 중 사용자가 언제든 상태를 조회(`Polling`)할 수 있도록 초고속 저장소 활용 |
> | **Notification** | SSE | 폴링에 의한 불필요한 네트워크 트래픽을 줄이고 사용자에게 즉각적인 당첨 경험 제공 |
>
> ### 4.6. 적용 효과 및 실측 데이터 (Experimental Results)
> ---
>   - **자원 효율**: DB 커넥션 점유 시간 급감 (스레드 차단 해제).
>   - **실제 처리 로그 (Raw Log)**:
>     ```bash
>     # 비동기 처리 상태 폴링 결과
>     - 시도 1: 현재 상태 -> PENDING (Kafka 적재 중)
>     - 시도 2: 현재 상태 -> SUCCESS (Consumer 처리 완료)
>     >> [SUCCESS] 예약 ID: 6 생성 완료
>     ```

<br><br>

## 향후 과제: 진정한 "기다림"의 미학
---

> [!NOTE]
> 지금까지의 구현은 "비동기 처리"에 집중되어 있습니다. 다음 단계에서는 사용자에게 진정한 대기 순번을 제공하는 시스템으로 진화합니다.
>
> - **Step 5: Redis Sorted Set 기반 대기열**
> - "내 앞에 몇 명이 남았는지" 실시간 순번 제공.
> - 트래픽 과부하 시 시스템 진입 자체를 제어하는 Throttling 구현.
>
> **Next Step**: Phase 3.5 - Redis Sorted Set을 이용한 실시간 대기 순번 피드백 시스템 구현 시작.

<br><br>

## Step 5: Redis Sorted Set 기반 실시간 대기열
---

> [!TIP]
> **핵심 가치**: 사용자에게 "내 앞의 대기자 수"를 실시간으로 피드백하여 UX를 혁신하고, 서버 인입량을 조절하는 중추적 역할을 수행합니다.
>
> ### 5.1. 개요 (Overview)
> - 단순히 Kafka에 요청을 쌓는 것만으로는 사용자의 불안감을 해소할 수 없습니다.
> - "내 앞에 5,000명이 대기 중입니다"와 같은 구체적인 정보 제공을 통해 사용자 경험(UX)을 혁신하는 것이 목표입니다.
>
> ### 5.2. 핵심 매커니즘: Redis Sorted Set (ZSET)
> - **Key**: `waiting-queue:concert:{concertId}` (콜론`:`을 사용하여 계층 구조 표현)
> - **Member**: `userId` (중복 불가능한 사용자의 식별자)
> - **Score**: `System.currentTimeMillis()` (시간 기반 자동 정렬)
> - **장점**: 이진 탐색 기반의 알고리즘($O(\log N)$)을 사용하여 수백만 명 중 내 순위를 즉시 조회 가능.
>
> ### 5.3. Redis Z-명령어 상세 및 Java 구현 (Deep Dive)
>
> | 명령어 | 설명 | Java 활용 예시 |
> | :--- | :--- | :--- |
> | **ZADD** | 대기열 진입 | `opsForZSet().add(key, userId, score)` |
> | **ZRANK** | 내 순위 조회 | `opsForZSet().rank(key, userId)` |
> | **ZRANGE** | 활성화 대상 추출 | `opsForZSet().range(key, start, stop)` |
> | **ZREM** | 대기열 탈출 | `opsForZSet().remove(key, userId)` |
> | **ZCARD** | 현재 대기자 수 | `opsForZSet().size(key)` |
>
> ### 5.4. 실제 구현 코드 (WaitingQueueServiceImpl)
>
> ```java
> @Service
> @RequiredArgsConstructor
> public class WaitingQueueServiceImpl implements WaitingQueueService {
>     private final StringRedisTemplate redisTemplate;
>     private static final String QUEUE_KEY_PREFIX = "waiting-queue:";
>     private static final String ACTIVE_KEY_PREFIX = "active-user:";
>
>     @Override
>     public WaitingQueueResponse join(Long userId, Long concertId) {
>         String queueKey = QUEUE_KEY_PREFIX + concertId;
>         String userIdStr = String.valueOf(userId);
>         if (Boolean.TRUE.equals(redisTemplate.hasKey(ACTIVE_KEY_PREFIX + userIdStr))) {
>             return WaitingQueueResponse.builder().userId(userId).concertId(concertId).status("ACTIVE").rank(0L).build();
>         }
>         redisTemplate.opsForZSet().add(queueKey, userIdStr, System.currentTimeMillis());
>         return getStatus(userId, concertId);
>     }
>
>     @Override
>     public WaitingQueueResponse getStatus(Long userId, Long concertId) {
>         String queueKey = QUEUE_KEY_PREFIX + concertId;
>         Long rank = redisTemplate.opsForZSet().rank(queueKey, String.valueOf(userId));
>         return WaitingQueueResponse.builder()
>                 .userId(userId).concertId(concertId)
>                 .status(rank != null ? "WAITING" : "NONE")
>                 .rank(rank != null ? rank + 1 : -1L).build();
>     }
> }
> ```
>
> ### 5.5. 검증 결과 및 실측 데이터 (Results & Impact)
> ---
>   - **대기열 시뮬레이션 데이터**:
>     | User ID | Status | Rank | 비고 |
>     | :--- | :--- | :---: | :--- |
>     | 101 | WAITING | 1 | 가장 먼저 진입 |
>     | 102 | WAITING | 2 | |
>     | 103 | WAITING | 3 | |
>     | 101 | ACTIVE | 0 | 스케줄러에 의해 활성화됨 |
>
>   - **효과**: 불필요한 새로고침 차단 및 대기 순번에 따른 심리적 안정감 제공.

## Step 6: 유입량 제어 전략 (Throttling)
---

> [!WARNING]
> **시스템 생존 보장**: 임계치 이상의 요청을 사전에 차단하고 활성화 인원을 동적으로 조절하는 최종 수비 단계입니다.
>
> ### 6.1. 배경: 대기열 우회 공격 (The Bypass Trap)
> ---
>   - **현상**: Step 5에서 프론트엔드 대기열을 구현했으나, 사용자가 예약 API(/api/v4/queue) 주소를 알아내어 직접 호출할 경우 대기열 로직이 무력화됨.
>   - **기술적 함정**: 대기열은 '안내'일 뿐 '강제'가 아닙니다. 백엔드에서 검증하지 않으면 수만 명의 '새치기' 요청이 DB 락 경합을 유발하여 시스템 전체가 마비됩니다.
>   - **엔지니어링 딜레마 (Filter vs Interceptor)**: 
>     - Filter는 서블릿 레벨에서 동작하여 Spring Context에 접근하기 번거롭습니다. 
>     - Interceptor는 스프링 빈(RedisTemplate 등) 접근이 용이하고, 핸들러 매핑 정보를 기반으로 특정 API 그룹만 정교하게 제어할 수 있어 이 시나리오에 더 적합합니다.
>
> ### 6.2. 코드 대조 (Before & After)
> ---
>
> #### Before: 검증 없는 무방비 상태
> ```java
> @PostMapping("/v4/queue")
> public ResponseEntity<String> reserve(@RequestBody ReservationRequest request) {
>     // 대기열을 거치지 않아도 Kafka로 요청이 발행됨 (위험)
>     kafkaProducer.send(ReservationEvent.of(request));
>     return ResponseEntity.accepted().build();
> }
> ```
>
> #### After: 인터셉터를 통한 진입 가드 적용
> ```java
> @Slf4j
> @Component
> @RequiredArgsConstructor
> public class WaitingQueueInterceptor implements HandlerInterceptor {
>     private final StringRedisTemplate redisTemplate;
>
>     @Override
>     public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception {
>         String userId = request.getHeader("User-Id");
>         if (userId == null || !redisTemplate.hasKey("active-user:" + userId)) {
>             log.warn(">>>> [Interceptor] 거부된 사용자: {}, 활성 토큰 없음", userId);
>             response.sendError(403, "Not an active user in waiting queue");
>             return false; // 비즈니스 로직(Kafka/DB) 진입 전 원천 차단
>         }
>         return true;
>     }
> }
> ```
>
> ### 6.3. 검증 결과 및 물리적 데이터 증거 (Experimental Results)
> ---
>   - **검증 방법**: `make test-v6` 실행 및 DB/Redis 상태 직접 조회.
>
> #### 실제 실행 로그 (Raw Log)
> ```bash
> >>>> [v6 Test] 유입량 제어 검증 시작...
> [Step 1] 테스트 유저 생성 중... 성공 (ID: 7)
> [Step 2] 유저 7 강제 활성화... 완료
> [Step 3] 활성 유저 요청 테스트... 성공 (Status: 200)
> [Step 4] 비활성 유저 차단 테스트... 성공 (Status: 403 Forbidden)
> ```
>
> #### 물리적 데이터 증거 (Hard Evidence)
> ```sql
> -- DB 예약 테이블 조회 결과
> SELECT id, user_id, seat_id FROM reservations ORDER BY id DESC LIMIT 1;
> -- Result: id: 7, user_id: 7, seat_id: 8 (인터셉터 통과 데이터 확인)
> ```
>
> ### 6.4. 핵심 교훈 및 트러블슈팅 (Lessons Learned)
> ---
>   - **"시스템은 스스로를 증명해야 한다"**: 클라이언트의 가이드를 신뢰하지 말고, 서버가 가진 상태(Redis)를 기준으로 요청의 유효성을 매번 증명해야 합니다.
>   - **리소스 절약**: 비싼 DB 트랜잭션이나 Kafka 메시지 발행 전에 초고속 In-memory(Redis) 조회를 통해 90% 이상의 부적절한 요청을 걸러내는 것이 고성능 시스템의 핵심입니다.

<br><br>

## 부록: 인프라 트러블슈팅 리포트 (Troubleshooting Guide)
---
> [!ATTENTION]
> 본 시스템 구축 중 발생한 치명적인 장애 사례와 해결책을 기록하여 재발을 방지합니다.
>
> ### 1. YAML 설정 오염 및 인덴트 오류
>   - **현상**: 서버 기동 시 `ParserException` 발생하며 즉시 종료.
>   - **원인**: `application.yml` 수정 중 중복된 루트 키(`spring:`) 삽입 및 들여쓰기 불일치.
>   - **해결**: YAML 정규화 도구(Linter)를 활용하고, 중복 키를 통합하여 설정 정합성 확보.
>
> ### 2. Docker 내부 네트워크 통신 불능
>   - **현상**: 비동기 예약 시 Kafka 브로커를 찾지 못해 `500 Internal Server Error` 발생.
>   - **원인**: `SPRING_PROFILES_ACTIVE=docker` 환경임에도 `localhost:9092`로 접속 시도.
>   - **해결**: 브로커 주소를 Docker 서비스명인 `kafka:9092`로 수정하여 컨테이너 간 통신로 확보.
>
> ### 3. Kafka 직렬화 전략 불일치
>   - **현상**: `ClassCastException` 발생하며 메시지 전송 실패.
>   - **원인**: 기본 `StringSerializer`를 사용하면서 객체(`ReservationEvent`)를 전송하려 함.
>   - **해결**: `JsonSerializer` 및 `JsonDeserializer`를 명시적으로 설정하여 객체 단위 통신 보장.

<br><br>

## Step 7: SSE 기반 실시간 순번 자동 푸시
---

> [!TIP]
> **사용자 경험의 완성**: 사용자가 직접 새로고침하거나 폴링하지 않아도, 순번이 바뀔 때마다 서버가 실시간으로 최신 정보를 밀어주는(Push) 완성형 대기열 시스템입니다.
>
> ### 7.1. Failure-First: 폴링 방식의 한계와 기술적 함정
> - **한계 1 (네트워크 낭비)**: 폴링 방식은 사용자가 많을수록 `GET /status` 호출이 폭증해 API/Redis 자원을 불필요하게 점유한다.
> - **한계 2 (상태 역전)**: 폴링 간격 사이에 `WAITING -> ACTIVE`가 발생하면 클라이언트는 늦게 반영되어 UX가 끊긴다.
> - **기술적 함정 1**: SSE 연결 직후 현재 상태 스냅샷을 보내지 않으면 화면이 빈 상태로 남는다.
> - **기술적 함정 2**: 동일 사용자 재연결 시 기존 emitter 정리가 없으면 메모리 누수/중복 전송이 발생한다.
> - **기술적 함정 3**: `ACTIVE` 이벤트에 TTL(`activeTtlSeconds`)을 포함하지 않으면 프론트가 예약 가능 시간을 계산할 수 없다.
>
> ### 7.2. Before & After: 구현 방식 비교
> #### Before (Polling 기반, Bad Practice)
> ```bash
> # 1초마다 상태 조회를 반복하는 방식
> while true; do
>   curl -s "http://localhost:8080/api/v1/waiting-queue/status?userId=700001&concertId=1"
>   sleep 1
> done
> ```
> - **문제점**: 상태가 안 바뀌어도 동일 요청이 반복되고, 대규모 동접 시 API 호출량이 선형 증가한다.
>
> #### After 1 (SSE 구독 + 즉시 스냅샷 전송)
> ```java
> @GetMapping(value = "/subscribe", produces = MediaType.TEXT_EVENT_STREAM_VALUE)
> public SseEmitter subscribe(@RequestParam Long userId, @RequestParam Long concertId) {
>     SseEmitter emitter = sseEmitterManager.subscribeQueue(userId, concertId);
>     WaitingQueueResponse currentStatus = waitingQueueService.getStatus(userId, concertId);
>     Long activeTtlSeconds = WaitingQueueStatus.ACTIVE.name().equals(currentStatus.getStatus())
>             ? waitingQueueService.getActiveTtlSeconds(userId) : 0L;
>
>     WaitingQueueSsePayload payload = WaitingQueueSsePayload.builder()
>             .userId(userId)
>             .concertId(concertId)
>             .status(currentStatus.getStatus())
>             .rank(currentStatus.getRank())
>             .activeTtlSeconds(activeTtlSeconds)
>             .timestamp(Instant.now().toString())
>             .build();
>
>     if (WaitingQueueStatus.ACTIVE.name().equals(currentStatus.getStatus())) {
>         sseEmitterManager.sendQueueActivated(userId, concertId, payload);
>     } else {
>         sseEmitterManager.sendQueueRankUpdate(userId, concertId, payload);
>     }
>     return emitter;
> }
> ```
>
> #### After 2 (스케줄러 이벤트 발행: `RANK_UPDATE` / `ACTIVE`)
> ```java
> @Scheduled(fixedDelayString = "${app.waiting-queue.activation-delay-millis}")
> public void activateWaitingUsers() {
>     Long concertId = properties.getActivationConcertId();
>     List<Long> activatedUsers = waitingQueueService.activateUsers(concertId, properties.getActivationBatchSize());
>     if (activatedUsers.isEmpty()) {
>         return;
>     }
>
>     Set<Long> activatedUserSet = new HashSet<>(activatedUsers);
>     for (Long userId : activatedUsers) {
>         Long activeTtlSeconds = waitingQueueService.getActiveTtlSeconds(userId);
>         sseEmitterManager.sendQueueActivated(
>                 userId, concertId,
>                 buildPayload(userId, concertId, WaitingQueueStatus.ACTIVE.name(), 0L, activeTtlSeconds)
>         );
>     }
>
>     publishRankUpdates(concertId, activatedUserSet);
> }
> ```
> - **개선점**: 상태 변화가 생긴 사용자에게만 푸시하므로, 불필요한 폴링 트래픽 없이 이벤트 중심으로 상태를 전달한다.
>
> ### 7.3. 데이터 계약 (SSE Payload Contract)
> ```java
> @Getter
> @NoArgsConstructor
> @AllArgsConstructor
> @Builder
> public class WaitingQueueSsePayload {
>     private Long userId;
>     private Long concertId;
>     private String status;
>     private Long rank;
>     private Long activeTtlSeconds;
>     private String timestamp;
> }
> ```
> - `status`: `WAITING`, `ACTIVE`, `NONE`
> - `rank`: 대기열 순번 (`ACTIVE`는 0)
> - `activeTtlSeconds`: 활성 상태 유효 시간(초)
>
> ### 7.4. Execution Log (Raw Evidence)
> #### 1차 회귀 실패 로그 (`prj-docs/api-test/latest.md`, 2026-02-11)
> ```bash
> [v7 Test] SSE 순번 자동 푸시 검증 시작...
> [Step 1] 대기열 진입 요청 ... 성공
> [Step 2] SSE 구독 시작 ... INIT/RANK_UPDATE/ACTIVE 이벤트 미수신
> 실패! INIT 이벤트 미수신
> No static resource api/v1/waiting-queue/subscribe.
> ```
>
> #### 원인 분석과 개선
> ```text
> 원인: run-step7-regression 스크립트가 docker compose up -d만 수행해
>      로컬 코드 변경이 컨테이너 이미지에 반영되지 않는 케이스가 발생.
> 개선: run-step7-regression에 --build + force recreate(down -> up) 절차를 추가.
> ```
>
> #### 2차 회귀 재실행 로그 (`prj-docs/api-test/latest.md`, 2026-02-11)
> ```text
> - Result: PASS
> - Passed: 1
> - Failed: 0
> - v7-sse-rank-push.sh: PASS (exit code 0)
> ```
>
> ### 7.5. 핵심 교훈 (Lessons Learned)
> - 폴링 제거만으로 끝나지 않는다. **연결 수명주기(재연결/타임아웃/heartbeat)**를 같이 설계해야 한다.
> - Step 7의 본질은 "이벤트를 보낸다"가 아니라, **현재 상태 스냅샷 + 상태 전이 이벤트 + TTL 계약**을 함께 일관되게 전달하는 것이다.
> - 운영 회귀 스크립트는 코드 변경 반영을 보장해야 한다. (`--build`/재생성 없이 돌리면 거짓 실패가 발생할 수 있다.)
> - 문서/HTTP/API 스크립트/실행 리포트를 함께 유지해야 운영 중 회귀를 빠르게 탐지할 수 있다.

<br><br>

## Step 8: k6 성능 기준선 확정 및 병목 제거
---

> [!TIP]
> **작업 전 이해 가이드**: Step 8은 프론트엔드 연동 단계가 아니라, `join/status/subscribe` 백엔드 경로를 정량 측정하고 병목을 제거하는 단계입니다.
>
> ### 8.1. Failure-First: 측정 없이 튜닝할 때 실패하는 이유
> - **기술적 함정 1 (평균값 착시)**: 평균 응답시간만 보면 tail latency(p95/p99) 급증 구간을 놓친다.
> - **기술적 함정 2 (부분 최적화)**: `join`만 개선하고 `status/subscribe`를 제외하면 실사용 체감이 개선되지 않는다.
> - **기술적 함정 3 (포화 지표 누락)**: DB 커넥션 풀, Redis, 스케줄러 큐 포화를 함께 보지 않으면 원인 추적이 왜곡된다.
> - **기술적 함정 4 (다변수 변경)**: 한 번에 여러 코드를 바꾸면 어떤 변경이 효과를 냈는지 입증할 수 없다.
> - **기술적 함정 5 (프론트 의존 오해)**: Step 8 검증은 `API_HOST` 기준 백엔드 부하 실험으로, 프론트가 없어도 실행 가능하다.
>
> ### 8.2. Before & After: 성능 개선 접근 방식
> #### Before (Bad Practice)
> - "느리다"는 체감만으로 즉시 코드를 수정하고, 재현 가능한 기준선 없이 효과를 판단한다.
> - 실패 시 원인 구분이 불가능해 "튜닝 반복"이 누적된다.
>
> #### After (Baseline-Driven Practice)
> - 기준선 측정: `make test-k6` 실행으로 현재 처리량/지연/오류율을 기록한다.
> - 가설 1개 선정: 병목 후보를 하나만 선택하고 백엔드 코드를 1건만 변경한다.
> - 재측정/비교: 동일 조건으로 재실행해 before/after 차이를 수치로 검증한다.
> - 문서화: `prj-docs/api-test/k6-latest.md`에 원인/개선 근거/잔여 리스크를 남긴다.
>
> ### 8.3. 측정 범위 (Step 8 SoT)
> - **핵심 API 경로**: `POST /api/v1/waiting-queue/join`, `GET /api/v1/waiting-queue/status`, `GET /api/v1/waiting-queue/subscribe`
> - **핵심 지표**: throughput(req/s), `http_req_duration p95/p99`, error rate, known status rate
> - **포화 지표**: DB pool active/waiting, Redis 응답 지연, 스케줄러 지연, SSE emitter 수 추이
> - **재현 명령**:
>   - `cd workspace/apps/backend/ticket-core-service`
>   - `make test-k6`
>   - 필요 시 `K6_VUS=120 K6_DURATION=120s make test-k6`
>
> ### 8.4. 백엔드 병목 후보 체크리스트 (개선 포인트)
> - `join` 경로에서 중복 Redis 조회/직렬화 비용이 과도하지 않은지 확인한다.
> - `status` 경로가 요청당 불필요한 다중 조회를 수행하지 않는지 점검한다.
> - `subscribe`/스케줄러 fan-out에서 구독자 수 증가 시 O(n) 병목이 발생하지 않는지 확인한다.
> - DB 인덱스/쿼리 플랜이 대기열 활성화 배치에 맞게 최적화됐는지 검증한다.
> - 스레드풀/스케줄러 설정이 실제 동시성 부하에 맞게 설정됐는지 점검한다.
>
> ### 8.5. Execution Log 템플릿 (작업 전 준비)
> - **Run Metadata**: `VUS`, `Duration`, `API_HOST`, 실행 시각, 커밋 SHA
> - **Before Metrics**: throughput, p95/p99, error rate, 병목 후보
> - **Change Set**: 수정 파일/설정, 변경 목적, 예상 효과
> - **After Metrics**: 동일 항목 재측정 결과
> - **판정**: 개선 성공/부분 개선/실패 + 다음 액션 1개
>
> ### 8.6. 완료 판정 (Definition of Done)
> - `prj-docs/api-test/k6-latest.md`에 before/after 수치와 병목 원인/개선 근거가 기록된다.
> - 성능 개선이 수치로 증명되지 않으면 Step 8은 완료로 보지 않는다.
>
> ### 8.7. Execution Log (Raw Evidence, 2026-02-11)
> ```bash
> cd workspace/apps/backend/ticket-core-service
> K6_VUS=20 K6_DURATION=300s make test-k6
> ```
>
> ```text
> - Result: PASS
> - http_reqs.count: 56700
> - http_req_failed.rate: 0
> - http_req_duration.p(95): 7.573ms
> - http_req_duration.p(99): 13.441ms
> - join_accepted.count: 390
> - join_rejected.count: 56310
> ```
>
> ### 8.8. Before & After (검증 파이프라인 개선)
> | 구분 | Before | After |
> | :--- | :--- | :--- |
> | k6 리포트 파일 | `k6-latest.md`가 0 bytes(지표 확인 불가) | `k6-latest.md`/`k6-summary.json` 생성 완료 |
> | Step 7 회귀 신뢰성 | 구버전 컨테이너 실행으로 거짓 실패 가능 | `--build` + 재생성으로 로컬 코드 기준 회귀 검증 |
> | 판단 가능성 | 실패 원인/수치 근거 부족 | PASS/지표/아티팩트 기반 의사결정 가능 |
>
> ### 8.9. 다음 개선 액션 (코드 레벨)
> - `status/subscribe` 경로의 쿼리/Redis 호출 횟수를 계측해 병목 구간을 먼저 식별한다.
> - 단일 변경(예: 조회 경로 캐시 또는 fan-out 최적화) 후 동일 조건(`20 VUs`, `300s`)으로 재측정한다.
> - `k6-latest.md`에 before/after diff를 누적해 Step 8 완료 판정을 갱신한다.
